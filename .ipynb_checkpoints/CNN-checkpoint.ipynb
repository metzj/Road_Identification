{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "This notebook contains a test CNN in pytorch, to get familiar with this developping environment. It also acts as a template for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2479c9831f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os,sys\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#seed for reproducible results\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch module\n",
    "\n",
    "This module contains the CNN named SimpleCNN.\n",
    "\n",
    "In \"init\", the layers have to be initialized (for instance conv layers, maxpool layers and fully connected layers. ReLu do not take hyperparameters, so it doesn't need an initialization.\n",
    "\n",
    "In \"forward\", the structure of the CNN is laid out by taking an input tensor x and applying the layers in the correct order to this tensor. This function is the forward pass of the CNN and returns the computed x.\n",
    "\n",
    "The backward pass can be computed by Pytorch's autograd function. For this to be achieved, our input tensor and our weights must be of type \"Variable\" (as imported above), this type stores changes to the tensor automatically which makes it possible to compute the gradient very easily. When declaring a Variable tensor, the option \"requires_grad\" must be set to True, otherwise the gradient will not be computed.\n",
    "\n",
    "### Module structure\n",
    "\n",
    "For this example module, we will define a fully convolutional neural network (FCN), with three convolutional layers and one transposed convolution (or deconvolution) to upsample the results of the convolutions.\n",
    "\n",
    "\n",
    "- Input image: 3 channels 400x400\n",
    "\n",
    "Three convolutions layers so that spatial stuff happen (every parameter is pretty arbitrary right now)\n",
    "- Convolution with kernel size 5, reduce to 1 channel (output  1 channel 396x396)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 198x198)\n",
    "- Convolution with kernel size 17 (output 182x182)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 91x91)\n",
    "- Convolution with kernel size 22 (output 70x70)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 35x35)\n",
    "\n",
    "A transposed convolution layer\n",
    "- Deconv with kernel size 400-35+1=366 (output 400x400)\n",
    "- Sigmoid for the binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_3conv_1deconv(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_3conv_1deconv, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 1, kernel_size=5)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(1, 1, kernel_size=17)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(1, 1, kernel_size=22)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.deconv = torch.nn.ConvTranspose2d(1, 2, kernel_size=366)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.deconv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model optimized with adam and cross entropy will converge to all-black images every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "(100, 3, 400, 400)\n",
      "Loading 100 images\n",
      "(100, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "# Loading a set of 100 training images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.array([load_image(image_dir + files[i]) for i in range(n)]).swapaxes(1,3).swapaxes(2,3)\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_input = imgs[0:15] #norm = 0:90\n",
    "validation_input = imgs[15:20] #norm = 90:100\n",
    "\n",
    "image_dir = root_dir + \"groundtruth/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_target = imgs[0:15] #norm = 0:90\n",
    "validation_target = imgs[15:20] #norm = 90:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep 10 images from this set as a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will optimize the cross-entropy loss using adam algorithm\n",
    "net = FCN_3conv_1deconv()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, n_epochs):\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for index in range(np.shape(train_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(train_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(train_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(input_image)\n",
    "            loss = loss_function(outputs, target_image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            print(\"Epoch\", epoch, \", image\", index, \", image loss:\", loss.item(), \", time elapsed:\", time.time() - training_start_time)\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for index in range(np.shape(validation_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(validation_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(validation_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(input_image)\n",
    "            val_loss = loss_function(val_outputs, target_image)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        print(\"Validation loss for epoch\", epoch, \":\", total_val_loss/np.shape(validation_input)[0])\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 0 , image loss: 0.693555474281311 , time elapsed: 3.7940659523010254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 1 , image loss: 0.6881590485572815 , time elapsed: 6.554765462875366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 2 , image loss: 0.679782509803772 , time elapsed: 9.274024724960327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 3 , image loss: 0.6683339476585388 , time elapsed: 11.993794441223145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3d0c0079e127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-8cc533b080a2>\u001b[0m in \u001b[0;36mtrainNet\u001b[1;34m(net, n_epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainNet(net, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.tensor(validation_input[0]).unsqueeze(0)\n",
    "target_image = torch.tensor(validation_target[0]).unsqueeze(0)\n",
    "           \n",
    "#Forward pass\n",
    "val_output = net(input_image)\n",
    "output_image = val_output[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target_image.squeeze(0), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(output_image.detach().numpy(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birth of the new model: balanced-dataset-compliant model\n",
    "The previously presented model is not truly fully convolutional, because the last layer of deconvolution is hardcoded to render a 400x400 output image. We would like to output a NxN image where NxN is also the size of the input. This is absolutely necessary to train a model using the balanced dataset since the balanced dataset is made of 100x100 and 200x200 images and will have to be tested on 400x400 images. We will fix the deconvolution layer and have convolutional layers that do not reduce the input as much because otherwise it would reject 100x100 images. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_3conv_1deconv_balanced(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_3conv_1deconv_balanced, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 1, kernel_size=5)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(1, 1, kernel_size=7)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(1, 1, kernel_size=12)\n",
    "        \n",
    "        self.deconv = torch.nn.ConvTranspose2d(1, 2, kernel_size=16)\n",
    "        self.upsample = torch.nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.deconv(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "As a first try, we will only train on the 100x100 images, and use some of the 100x100 images as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_images_100 = np.load('balanced_dataset/sat_images_100.npy').astype(np.float64).swapaxes(1,3).swapaxes(2,3) #1752 images\n",
    "gt_images_100 = np.load('balanced_dataset/groundtruth_100.npy').astype(np.float64)\n",
    "\n",
    "# We will take 1500 images as input, and the remaining 252 images as validation set\n",
    "train_input = sat_images_100[:1500]\n",
    "validation_input = sat_images_100[1500:]\n",
    "\n",
    "train_target = gt_images_100[:1500]\n",
    "validation_target = gt_images_100[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will optimize the cross-entropy loss using adam algorithm\n",
    "net = FCN_3conv_1deconv_balanced()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, n_epochs):\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for index in range(np.shape(train_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(train_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(train_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(input_image.float())\n",
    "            loss = loss_function(outputs, target_image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch\", epoch, \", training loss:\", loss.item(), \", time elapsed:\", time.time() - training_start_time)\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for index in range(np.shape(validation_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(validation_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(validation_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(input_image.float())\n",
    "            val_loss = loss_function(val_outputs, target_image)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        print(\"Validation loss for epoch\", epoch, \":\", total_val_loss/np.shape(validation_input)[0])\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Remi\\Miniconda3\\envs\\ntds_2018\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , training loss: 0.32080474495887756 , time elapsed: 11.858800411224365\n",
      "Validation loss for epoch 0 : 0.32080450654029846\n",
      "Epoch 1 , training loss: 0.3173049986362457 , time elapsed: 24.934110641479492\n",
      "Validation loss for epoch 1 : 0.31730493903160095\n",
      "Epoch 2 , training loss: 0.31542474031448364 , time elapsed: 37.95851230621338\n",
      "Validation loss for epoch 2 : 0.31542474031448364\n",
      "Epoch 3 , training loss: 0.3144344687461853 , time elapsed: 50.99654006958008\n",
      "Validation loss for epoch 3 : 0.3144344687461853\n",
      "Epoch 4 , training loss: 0.3139001727104187 , time elapsed: 64.10203456878662\n",
      "Validation loss for epoch 4 : 0.3139001429080963\n",
      "Epoch 5 , training loss: 0.31355535984039307 , time elapsed: 77.22469711303711\n",
      "Validation loss for epoch 5 : 0.31355535984039307\n",
      "Epoch 6 , training loss: 0.31346067786216736 , time elapsed: 90.28439927101135\n",
      "Validation loss for epoch 6 : 0.31346067786216736\n",
      "Epoch 7 , training loss: 0.3133968412876129 , time elapsed: 103.39106678962708\n",
      "Validation loss for epoch 7 : 0.3133968114852905\n",
      "Epoch 8 , training loss: 0.3132960796356201 , time elapsed: 117.19144201278687\n",
      "Validation loss for epoch 8 : 0.3132960796356201\n",
      "Epoch 9 , training loss: 0.31325215101242065 , time elapsed: 130.32565665245056\n",
      "Validation loss for epoch 9 : 0.31325215101242065\n",
      "Training finished, took 131.57s\n"
     ]
    }
   ],
   "source": [
    "trainNet(net, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Remi\\Miniconda3\\envs\\ntds_2018\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247b7a9c438>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = torch.tensor(validation_input[3]).unsqueeze(0)\n",
    "target_image = torch.tensor(validation_target[3])\n",
    "           \n",
    "#Forward pass\n",
    "val_output = net(input_image.float())\n",
    "output_image = val_output[0,1]\n",
    "\n",
    "plt.imshow(target_image, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x247b7782320>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output_image.detach().numpy(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"test_set_images/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.array([load_image(image_dir + files[i]) for i in range(n)]).swapaxes(1,3).swapaxes(2,3)\n",
    "print(np.shape(imgs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

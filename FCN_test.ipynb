{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN\n",
    "This notebook contains a test CNN in pytorch, to get familiar with this developping environment. It also acts as a template for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e6b72e4bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os,sys\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#seed for reproducible results\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch module\n",
    "\n",
    "This module contains the FCN based on the CNN module\n",
    "\n",
    "### Module structure\n",
    "\n",
    "For this module, we will try to implement the complex diagram describe in http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Zhou_D-LinkNet_LinkNet_With_CVPR_2018_paper.pdf\n",
    "\n",
    "Usefull links:\n",
    "\n",
    "-Torch documentations (especially for the input/ouput size of Conv2d and ConvTranspose2d: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "-This link to better understand what each argument in Conv2d and ConvTranspose2d: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class takes our input of size 400*400 and enlarges it to size 512*512\n",
    "\n",
    "class FCN_DLinkNet(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_DLinkNet, self).__init__()\n",
    "        \n",
    "        # Encoding part\n",
    "        \n",
    "        self.convstart = torch.nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=58)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.convEncodBlock1 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock2start = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock3start = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock3 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock4start = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock4 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Center part\n",
    "        \n",
    "        self.convCenterdil1 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1) #same as convEncodBlock4 but added for clarity\n",
    "        self.convCenterdil2 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.convCenterdil4 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=4, dilation=4)\n",
    "        self.convCenterdil8 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=8, dilation=8)\n",
    "        \n",
    "        # Decoding part\n",
    "        \n",
    "        self.convDecodBlock4start = torch.nn.Conv2d(512, 128, kernel_size=1)\n",
    "        self.deconvDecodBlock4 = torch.nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock4end = torch.nn.Conv2d(128, 256, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock3start = torch.nn.Conv2d(256, 64, kernel_size=1)\n",
    "        self.deconvDecodBlock3 = torch.nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock3end = torch.nn.Conv2d(64, 128, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock2start = torch.nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.deconvDecodBlock2 = torch.nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock2end = torch.nn.Conv2d(32, 64, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock1start = torch.nn.Conv2d(64, 16, kernel_size=1)\n",
    "        self.deconvDecodBlock1 = torch.nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock1end = torch.nn.Conv2d(16, 64, kernel_size=1)\n",
    "        \n",
    "        self.deconvDecodend = torch.nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=57)\n",
    "        self.convDecoddil1end = torch.nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        def codeBlock1(self, x):\n",
    "            for i in range(0, 3):\n",
    "                x_inter = F.relu(self.convEncodBlock1(x))\n",
    "                x_inter = F.relu(self.convEncodBlock1(x_inter))\n",
    "                # Moyenne\n",
    "                x = (x + x_inter)/2\n",
    "            return x\n",
    "\n",
    "        def codeBlock2(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock2start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock2(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :64, :, :] = x\n",
    "            # Moyenne\n",
    "            x = (new_x + x_inter)/2\n",
    "            for i in range(0, 3):\n",
    "                x_inter = F.relu(self.convEncodBlock2(x))\n",
    "                x_inter = F.relu(self.convEncodBlock2(x_inter))\n",
    "                # Moyenne\n",
    "                x = (x + x_inter)/2\n",
    "            return x\n",
    "\n",
    "        def codeBlock3(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock3start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock3(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :128, :, :] = x\n",
    "            # Moyenne\n",
    "            x = (new_x + x_inter)/2\n",
    "            for i in range(0, 5):\n",
    "                x_inter = F.relu(self.convEncodBlock3(x))\n",
    "                x_inter = F.relu(self.convEncodBlock3(x_inter))\n",
    "                # Moyenne\n",
    "                x = (x + x_inter)/2\n",
    "            return x\n",
    "\n",
    "        def codeBlock4(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock4start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock4(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :256, :, :] = x\n",
    "            # Moyenne\n",
    "            x = (new_x + x_inter)/2\n",
    "            for i in range(0, 2):\n",
    "                x_inter = F.relu(self.convEncodBlock4(x))\n",
    "                x_inter = F.relu(self.convEncodBlock4(x_inter))\n",
    "                # Moyenne\n",
    "                x = (x + x_inter)/2\n",
    "            return x\n",
    "\n",
    "        def decodeBlock4(self, x):\n",
    "            x = F.relu(self.convDecodBlock4start(x))\n",
    "            x = self.deconvDecodBlock4(x)\n",
    "            x = F.relu(self.convDecodBlock4end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock3(self, x):\n",
    "            x = F.relu(self.convDecodBlock3start(x))\n",
    "            x = self.deconvDecodBlock3(x)\n",
    "            x = F.relu(self.convDecodBlock3end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock2(self, x):\n",
    "            x = F.relu(self.convDecodBlock2start(x))\n",
    "            x = self.deconvDecodBlock2(x)\n",
    "            x = F.relu(self.convDecodBlock2end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock1(self, x):\n",
    "            x = F.relu(self.convDecodBlock1start(x))\n",
    "            x = self.deconvDecodBlock1(x)\n",
    "            x = F.relu(self.convDecodBlock1end(x))\n",
    "            return x\n",
    "        \n",
    "        x_start = F.relu(self.convstart(x))\n",
    "        x_start = self.pool(x_start)\n",
    "        \n",
    "        x_codeBlock1 = codeBlock1(self, x_start)\n",
    "        x_codeBlock2 = codeBlock2(self, x_codeBlock1)\n",
    "        x_codeBlock3 = codeBlock3(self, x_codeBlock2)\n",
    "        x_codeBlock4 = codeBlock4(self, x_codeBlock3)\n",
    "        \n",
    "        x_center1 = F.relu(self.convCenterdil1(x_codeBlock4))\n",
    "        x_center2 = F.relu(self.convCenterdil2(x_center1))\n",
    "        x_center3 = F.relu(self.convCenterdil4(x_center2))\n",
    "        x_center4 = F.relu(self.convCenterdil8(x_center3))\n",
    "        \n",
    "        x_center_final = (x_codeBlock4 + x_center1 + x_center2 + x_center3 + x_center4)/5\n",
    "        \n",
    "        x_decodeBlock4 = decodeBlock4(self, x_center_final)\n",
    "        x_decodeBlock3 = decodeBlock3(self, (x_decodeBlock4 + x_codeBlock3)/2)\n",
    "        x_decodeBlock2 = decodeBlock2(self, (x_decodeBlock3 + x_codeBlock2)/2)\n",
    "        x_decodeBlock1 = decodeBlock1(self, (x_decodeBlock2 + x_codeBlock1)/2)\n",
    "        \n",
    "        x_end = self.deconvDecodend(x_decodeBlock1)\n",
    "        x_end = F.relu(self.convDecoddil1end(x_end))\n",
    "        x_end = self.sigmoid(x_end)\n",
    "        \n",
    "        return(x_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class takes our input of size 400*400 and directly put it to size 256*256\n",
    "\n",
    "class FCN_DLinkNet_new(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_DLinkNet_new, self).__init__()\n",
    "        \n",
    "        # Encoding part\n",
    "        \n",
    "        self.convstart = torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=57)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.convEncodBlock1 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock2start = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock3start = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock3 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.convEncodBlock4start = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.convEncodBlock4 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Center part\n",
    "        \n",
    "        self.convCenterdil1 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1) #same as convEncodBlock4 but added for clarity\n",
    "        self.convCenterdil2 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.convCenterdil4 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=4, dilation=4)\n",
    "        self.convCenterdil8 = torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=8, dilation=8)\n",
    "        \n",
    "        # Decoding part\n",
    "        \n",
    "        self.convDecodBlock4start = torch.nn.Conv2d(512, 128, kernel_size=1)\n",
    "        self.deconvDecodBlock4 = torch.nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock4end = torch.nn.Conv2d(128, 256, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock3start = torch.nn.Conv2d(256, 64, kernel_size=1)\n",
    "        self.deconvDecodBlock3 = torch.nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock3end = torch.nn.Conv2d(64, 128, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock2start = torch.nn.Conv2d(128, 32, kernel_size=1)\n",
    "        self.deconvDecodBlock2 = torch.nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock2end = torch.nn.Conv2d(32, 64, kernel_size=1)\n",
    "        \n",
    "        self.convDecodBlock1start = torch.nn.Conv2d(64, 16, kernel_size=1)\n",
    "        self.deconvDecodBlock1 = torch.nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.convDecodBlock1end = torch.nn.Conv2d(16, 64, kernel_size=1)\n",
    "        \n",
    "        self.deconvDecodend = torch.nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=57)\n",
    "        self.convDecoddil1end = torch.nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        def codeBlock1(self, x):\n",
    "            for i in range(0, 3):\n",
    "                x_inter = F.relu(self.convEncodBlock1(x))\n",
    "                x_inter = F.relu(self.convEncodBlock1(x_inter))\n",
    "                # Moyenne\n",
    "                x = x + x_inter\n",
    "            return x\n",
    "\n",
    "        def codeBlock2(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock2start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock2(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :64, :, :] = x\n",
    "            # Moyenne\n",
    "            x = new_x + x_inter\n",
    "            for i in range(0, 3):\n",
    "                x_inter = F.relu(self.convEncodBlock2(x))\n",
    "                x_inter = F.relu(self.convEncodBlock2(x_inter))\n",
    "                # Moyenne\n",
    "                x = x + x_inter\n",
    "            return x\n",
    "\n",
    "        def codeBlock3(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock3start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock3(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :128, :, :] = x\n",
    "            # Moyenne\n",
    "            x = new_x + x_inter\n",
    "            for i in range(0, 5):\n",
    "                x_inter = F.relu(self.convEncodBlock3(x))\n",
    "                x_inter = F.relu(self.convEncodBlock3(x_inter))\n",
    "                # Moyenne\n",
    "                x = x + x_inter\n",
    "            return x\n",
    "\n",
    "        def codeBlock4(self, x):\n",
    "            x = self.pool(x)\n",
    "            x_inter = F.relu(self.convEncodBlock4start(x))\n",
    "            x_inter = F.relu(self.convEncodBlock4(x_inter))\n",
    "            new_x = torch.zeros(x_inter.shape)\n",
    "            new_x[:, :256, :, :] = x\n",
    "            # Moyenne\n",
    "            x = new_x + x_inter\n",
    "            for i in range(0, 2):\n",
    "                x_inter = F.relu(self.convEncodBlock4(x))\n",
    "                x_inter = F.relu(self.convEncodBlock4(x_inter))\n",
    "                # Moyenne\n",
    "                x = x + x_inter\n",
    "            return x\n",
    "\n",
    "        def decodeBlock4(self, x):\n",
    "            x = F.relu(self.convDecodBlock4start(x))\n",
    "            x = self.deconvDecodBlock4(x)\n",
    "            x = F.relu(self.convDecodBlock4end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock3(self, x):\n",
    "            x = F.relu(self.convDecodBlock3start(x))\n",
    "            x = self.deconvDecodBlock3(x)\n",
    "            x = F.relu(self.convDecodBlock3end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock2(self, x):\n",
    "            x = F.relu(self.convDecodBlock2start(x))\n",
    "            x = self.deconvDecodBlock2(x)\n",
    "            x = F.relu(self.convDecodBlock2end(x))\n",
    "            return x\n",
    "\n",
    "        def decodeBlock1(self, x):\n",
    "            x = F.relu(self.convDecodBlock1start(x))\n",
    "            x = self.deconvDecodBlock1(x)\n",
    "            x = F.relu(self.convDecodBlock1end(x))\n",
    "            return x\n",
    "        \n",
    "        x_start = F.relu(self.convstart(x))\n",
    "        \n",
    "        x_codeBlock1 = codeBlock1(self, x_start)\n",
    "        x_codeBlock2 = codeBlock2(self, x_codeBlock1)\n",
    "        x_codeBlock3 = codeBlock3(self, x_codeBlock2)\n",
    "        x_codeBlock4 = codeBlock4(self, x_codeBlock3)\n",
    "        \n",
    "        x_center1 = F.relu(self.convCenterdil1(x_codeBlock4))\n",
    "        x_center2 = F.relu(self.convCenterdil2(x_center1))\n",
    "        x_center3 = F.relu(self.convCenterdil4(x_center2))\n",
    "        x_center4 = F.relu(self.convCenterdil8(x_center3))\n",
    "        \n",
    "        x_center_final = x_codeBlock4 + x_center1 + x_center2 + x_center3 + x_center4\n",
    "        \n",
    "        x_decodeBlock4 = decodeBlock4(self, x_center_final)\n",
    "        x_decodeBlock3 = decodeBlock3(self, x_decodeBlock4 + x_codeBlock3)\n",
    "        x_decodeBlock2 = decodeBlock2(self, x_decodeBlock3 + x_codeBlock2)\n",
    "        x_decodeBlock1 = decodeBlock1(self, x_decodeBlock2 + x_codeBlock1)\n",
    "        \n",
    "        x_end = self.deconvDecodend(x_decodeBlock1)\n",
    "        x_end = F.relu(self.convDecoddil1end(x_end))\n",
    "        x_end = self.sigmoid(x_end)\n",
    "        \n",
    "        return(x_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model optimized with adam and cross entropy will converge to all-black images every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "(100, 3, 400, 400)\n",
      "Loading 100 images\n",
      "(100, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "# Loading a set of 100 training images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.array([load_image(image_dir + files[i]) for i in range(n)]).swapaxes(1,3).swapaxes(2,3)\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_input = imgs[0:12] #normally = 0:90\n",
    "validation_input = imgs[12:15] #normally = 90:100\n",
    "\n",
    "image_dir = root_dir + \"groundtruth/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_target = imgs[0:12] #normally = 0:90\n",
    "validation_target = imgs[12:15] #normally = 90:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep 10 images from this set as a validation set.\n",
    "To shorten the computationnal time we will use a smaller amount of images to do tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will optimize the cross-entropy loss using adam algorithm\n",
    "net = FCN_DLinkNet_new()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, n_epochs):\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for index in range(np.shape(train_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(train_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(train_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(input_image)\n",
    "            loss = loss_function(outputs, target_image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            print(\"Epoch\", epoch, \", image\", index, \", image loss:\", loss.item(), \", time elapsed:\", time.time() - training_start_time)\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for index in range(np.shape(validation_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(validation_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(validation_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(input_image)\n",
    "            val_loss = loss_function(val_outputs, target_image)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        print(\"Validation loss for epoch\", epoch, \":\", total_val_loss/np.shape(validation_input)[0])\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 0 , image loss: 0.6935377717018127 , time elapsed: 10.525941848754883\n",
      "Epoch 0 , image 1 , image loss: 0.6874483823776245 , time elapsed: 20.921140909194946\n",
      "Epoch 0 , image 2 , image loss: 0.6776967644691467 , time elapsed: 32.44532036781311\n",
      "Epoch 0 , image 3 , image loss: 0.6640986204147339 , time elapsed: 44.23080039024353\n",
      "Epoch 0 , image 4 , image loss: 0.6458911895751953 , time elapsed: 55.71009969711304\n",
      "Epoch 0 , image 5 , image loss: 0.47793036699295044 , time elapsed: 68.00920701026917\n",
      "Epoch 0 , image 6 , image loss: 0.47387319803237915 , time elapsed: 80.28736996650696\n",
      "Epoch 0 , image 7 , image loss: 0.4738675355911255 , time elapsed: 91.32983684539795\n",
      "Epoch 0 , image 8 , image loss: 0.4738675355911255 , time elapsed: 103.56611180305481\n",
      "Epoch 0 , image 9 , image loss: 0.4738675355911255 , time elapsed: 114.29142737388611\n",
      "Epoch 0 , image 10 , image loss: 0.4738675355911255 , time elapsed: 125.94027280807495\n",
      "Epoch 0 , image 11 , image loss: 0.4738675355911255 , time elapsed: 136.7772901058197\n",
      "Validation loss for epoch 0 : 0.4738675355911255\n",
      "Training finished, took 153.81s\n"
     ]
    }
   ],
   "source": [
    "trainNet(net, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.tensor(validation_input[0]).unsqueeze(0)\n",
    "target_image = torch.tensor(validation_target[0]).unsqueeze(0)\n",
    "           \n",
    "#Forward pass\n",
    "val_output = net(input_image)\n",
    "output_image = val_output[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e69542a898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target_image.squeeze(0), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e69c72f0b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output_image.detach().numpy(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

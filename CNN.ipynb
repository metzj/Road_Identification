{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "This notebook contains a test CNN in pytorch, to get familiar with this developping environment. It also acts as a template for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b4ddf2cc70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os,sys\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#seed for reproducible results\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch module\n",
    "\n",
    "This module contains the CNN named SimpleCNN.\n",
    "\n",
    "In \"init\", the layers have to be initialized (for instance conv layers, maxpool layers and fully connected layers. ReLu do not take hyperparameters, so it doesn't need an initialization.\n",
    "\n",
    "In \"forward\", the structure of the CNN is laid out by taking an input tensor x and applying the layers in the correct order to this tensor. This function is the forward pass of the CNN and returns the computed x.\n",
    "\n",
    "The backward pass can be computed by Pytorch's autograd function. For this to be achieved, our input tensor and our weights must be of type \"Variable\" (as imported above), this type stores changes to the tensor automatically which makes it possible to compute the gradient very easily. When declaring a Variable tensor, the option \"requires_grad\" must be set to True, otherwise the gradient will not be computed.\n",
    "\n",
    "### Module structure\n",
    "\n",
    "For this example module, we will define a fully convolutional neural network (FCN), with three convolutional layers and one transposed convolution (or deconvolution) to upsample the results of the convolutions.\n",
    "\n",
    "\n",
    "- Input image: 3 channels 400x400\n",
    "\n",
    "Three convolutions layers so that spatial stuff happen (every parameter is pretty arbitrary right now)\n",
    "- Convolution with kernel size 5, reduce to 1 channel (output  1 channel 396x396)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 198x198)\n",
    "- Convolution with kernel size 17 (output 182x182)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 91x91)\n",
    "- Convolution with kernel size 22 (output 70x70)\n",
    "- ReLu\n",
    "- MaxPool with kernel size 2 (output 35x35)\n",
    "\n",
    "A transposed convolution layer\n",
    "- Deconv with kernel size 400-35+1=366 (output 400x400)\n",
    "- Sigmoid for the binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_3conv_1deconv(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_3conv_1deconv, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 1, kernel_size=5)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(1, 1, kernel_size=17)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(1, 1, kernel_size=22)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.deconv = torch.nn.ConvTranspose2d(1, 2, kernel_size=366)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.deconv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model optimized with adam and cross entropy will converge to all-black images every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "(100, 3, 400, 400)\n",
      "Loading 100 images\n",
      "(100, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "# Loading a set of 100 training images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.array([load_image(image_dir + files[i]) for i in range(n)]).swapaxes(1,3).swapaxes(2,3)\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_input = imgs[0:15] #norm = 0:90\n",
    "validation_input = imgs[15:20] #norm = 90:100\n",
    "\n",
    "image_dir = root_dir + \"groundtruth/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(100, len(files)) # Load maximum 100 images\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(np.shape(imgs))\n",
    "\n",
    "train_target = imgs[0:15] #norm = 0:90\n",
    "validation_target = imgs[15:20] #norm = 90:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep 10 images from this set as a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will optimize the cross-entropy loss using adam algorithm\n",
    "net = FCN_3conv_1deconv()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, n_epochs):\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for index in range(np.shape(train_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(train_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(train_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(input_image)\n",
    "            loss = loss_function(outputs, target_image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            print(\"Epoch\", epoch, \", image\", index, \", image loss:\", loss.item(), \", time elapsed:\", time.time() - training_start_time)\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for index in range(np.shape(validation_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(validation_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(validation_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(input_image)\n",
    "            val_loss = loss_function(val_outputs, target_image)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        print(\"Validation loss for epoch\", epoch, \":\", total_val_loss/np.shape(validation_input)[0])\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 0 , image loss: 0.693555474281311 , time elapsed: 3.7940659523010254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 1 , image loss: 0.6881590485572815 , time elapsed: 6.554765462875366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 2 , image loss: 0.679782509803772 , time elapsed: 9.274024724960327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , image 3 , image loss: 0.6683339476585388 , time elapsed: 11.993794441223145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 400])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3d0c0079e127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-8cc533b080a2>\u001b[0m in \u001b[0;36mtrainNet\u001b[1;34m(net, n_epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainNet(net, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.tensor(validation_input[0]).unsqueeze(0)\n",
    "target_image = torch.tensor(validation_target[0]).unsqueeze(0)\n",
    "           \n",
    "#Forward pass\n",
    "val_output = net(input_image)\n",
    "output_image = val_output[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target_image.squeeze(0), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(output_image.detach().numpy(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birth of the new model: balanced-dataset-compliant model\n",
    "The previously presented model is not truly fully convolutional, because the last layer of deconvolution is hardcoded to render a 400x400 output image. We would like to output a NxN image where NxN is also the size of the input. This is absolutely necessary to train a model using the balanced dataset since the balanced dataset is made of 100x100 and 200x200 images and will have to be tested on 400x400 images. We will fix the deconvolution layer and have convolutional layers that do not reduce the input as much because otherwise it would reject 100x100 images. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN_3conv_1deconv_balanced(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(FCN_3conv_1deconv_balanced, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 1, kernel_size=5)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(1, 1, kernel_size=7)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(1, 1, kernel_size=12)\n",
    "        \n",
    "        self.deconv = torch.nn.ConvTranspose2d(1, 2, kernel_size=16)\n",
    "        self.upsample = torch.nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = self.deconv(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "As a first try, we will only train on the 100x100 images, and use some of the 100x100 images as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_images_100 = np.load('balanced_dataset/sat_images_100.npy').astype(np.float64).swapaxes(1,3).swapaxes(2,3) #1752 images\n",
    "gt_images_100 = np.load('balanced_dataset/groundtruth_100.npy').astype(np.float64)\n",
    "\n",
    "# We will take 1500 images as input, and the remaining 252 images as validation set\n",
    "train_input = sat_images_100[:1500]\n",
    "validation_input = sat_images_100[1500:]\n",
    "\n",
    "train_target = gt_images_100[:1500]\n",
    "validation_target = gt_images_100[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will optimize the cross-entropy loss using adam algorithm\n",
    "net = FCN_3conv_1deconv_balanced()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, n_epochs):\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for index in range(np.shape(train_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(train_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(train_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(input_image.float())\n",
    "            loss = loss_function(outputs, target_image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(\"Epoch\", epoch, \", training loss:\", loss.item(), \", time elapsed:\", time.time() - training_start_time)\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for index in range(np.shape(validation_input)[0]):\n",
    "            \n",
    "            input_image = Variable(torch.tensor(validation_input[index], requires_grad=True).unsqueeze(0))\n",
    "            target_image = Variable(torch.tensor(validation_target[index], dtype=torch.long).unsqueeze(0))\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(input_image.float())\n",
    "            val_loss = loss_function(val_outputs, target_image)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "        print(\"Validation loss for epoch\", epoch, \":\", total_val_loss/np.shape(validation_input)[0])\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , training loss: 0.3132411241531372 , time elapsed: 10.793042182922363\n",
      "Validation loss for epoch 0 : 0.3132411241531372\n",
      "Epoch 1 , training loss: 0.3132389783859253 , time elapsed: 23.449750900268555\n",
      "Validation loss for epoch 1 : 0.3132389783859253\n",
      "Epoch 2 , training loss: 0.31323811411857605 , time elapsed: 36.61889147758484\n",
      "Validation loss for epoch 2 : 0.31323811411857605\n",
      "Training finished, took 37.51s\n"
     ]
    }
   ],
   "source": [
    "trainNet(net, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b480576be0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = torch.tensor(validation_input[3]).unsqueeze(0)\n",
    "target_image = torch.tensor(validation_target[3])\n",
    "           \n",
    "#Forward pass\n",
    "val_output = net(input_image.float())\n",
    "output_image = val_output[0,1]\n",
    "\n",
    "plt.imshow(target_image, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b4802dbba8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAN4UlEQVR4nO3dW4yc5X3H8e9/jwaWle1wcrxQA1qRokgJ0apAyEUEiUtIFLggKmkquRWVb9qGppXC0l7lolItRQEuqkoIGqEqCqQGAbIiIovQqjd1s0APxga8BAQLy8EyYFyBvfb+ezHvrBeY9Y53DjvL8/1Io3ff81+P/Jvned6ZXUdmIunTr2+1C5DUHYZdKoRhlwph2KVCGHapEIZdKkRLYY+I6yPi+YiYjojJdhUlqf1ipZ+zR0Q/8ALwdWAG+A3w3czc177yJLXLQAvn/h4wnZm/BYiIB4AbgSXDfs455+SWLVt44403AHjvvfcW9h09ehSAEydOAFB/E/JLPypZRHxk2d/fD8Dw8PDCMaOjowBs2rSJl19+mYMHD0aja7US9s3Aq4vWZ4ArGxS7HdgOcNFFFzE1NcWOHTsAePzxxxeOe+GFF4CTbwBzc3MAzM/Pf2TZKPy+IWgtqge40baPh3twcBA4Gezx8fGFc7Zu3QrA5OQkV175iQguaCXsjd49PpG6zLwHuAdgbGwsd+zYwSOPPALA9PT0wnFHjhwBPhnyRddpoVSp95yq46qHvZ6D+oi3vty37+QAuj4qBpidnV3yfq08oJsBLly0Pga83sL1JHVQKw/oBqg9oLsOeI3aA7o/zMxnlzrn7LPPzomJCfbu3QvA4cOHF/YdP34c+GSPLumkvr5a/zwwcHJQXh/aX3755Tz99NO8//777Z2zZ+bxiPhz4FdAP/BPpwq6pNXVypydzPwl8Ms21SKpg1Y8jF+JoaGhPPfcczl06BAAx44dW9jn8F1qXn04DzA0NATAhg0bOHjwIMeOHWs4jPfrslIhutqz9/f35xlnnMGHH34InPwYQdLK1T+LX7duHR988AEnTpywZ5dK1tIDutOVmczNzTk/l9qonqe5ublTfvnMnl0qRFfDnpn26lKHzM/P27NL6vKcHZZ/95F0eup5Wm7UbM8uFaLrYbdXlzpjuWzZs0uFMOxSIQy7VAjDLhXCB3TSp4QP6CQBhl0qhmGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCrFs2CPiwoh4MiL2R8SzEXFbtX1jROyOiAPVckPny5W0UrHcH6mLiE3Apsx8OiLOBp4CbgL+GDiUmX8fEZPAhsy8fZlr+dcmpQ7LzGi0fdmePTNnM/Pp6uf3gf3AZuBG4P7qsPupvQFI6lGn9b+4RsQW4ApgD3B+Zs5C7Q0hIs5b4pztwPbWypTUqmWH8QsHRowA/wb8XWY+HBHvZub6RfvfycxTztsdxkudt+JhPEBEDAIPAT/LzIerzW9W8/n6vP6tdhQqqTOaeRofwH3A/sz8yaJdjwHbqp+3AY+2vzxJ7dLM0/ivAP8O/C8wX23+G2rz9l8AFwGvAN/JzEPLXMthvNRhSw3jm56zt4NhlzqvpTm7pLXPsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFaDrsEdEfEc9ExK5q/eKI2BMRByLiwYgY6lyZklp1Oj37bcD+Res7gDszcxx4B7i1nYVJaq+mwh4RY8A3gXur9QCuBXZWh9wP3NSJAiW1R7M9+13AD4H5av0zwLuZebxanwE2NzoxIrZHxFRETLVUqaSWLBv2iPgW8FZmPrV4c4NDs9H5mXlPZk5k5sQKa5TUBgNNHHMN8O2IuAFYB4xS6+nXR8RA1buPAa93rkxJrVq2Z8/MOzJzLDO3ALcAv87M7wFPAjdXh20DHu1YlZJa1srn7LcDfxUR09Tm8Pe1pyRJnRCZDafanblZRPduJhUqMxs9U/MbdFIpDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFaKpsEfE+ojYGRHPRcT+iLg6IjZGxO6IOFAtN3S6WEkr12zPfjfweGZ+DvgCsB+YBJ7IzHHgiWpdUo+KzDz1ARGjwH8Dl+SigyPieeCrmTkbEZuAf83My5a51qlvJqllmRmNtjfTs18CvA38NCKeiYh7I+Is4PzMnK0uPguc1+jkiNgeEVMRMbXC2iW1QTM9+wTwH8A1mbknIu4GDgN/kZnrFx33Tmaect5uzy51Xis9+wwwk5l7qvWdwJeAN6vhO9XyrXYUKqkzlg17Zr4BvBoR9fn4dcA+4DFgW7VtG/BoRyqU1BbLDuMBIuKLwL3AEPBb4E+ovVH8ArgIeAX4TmYeWuY6DuOlDltqGN9U2NvFsEud18qcXdKngGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCtFU2CPiBxHxbETsjYifR8S6iLg4IvZExIGIeDAihjpdrKSVWzbsEbEZ+D4wkZmfB/qBW4AdwJ2ZOQ68A9zayUIltabZYfwAcEZEDABnArPAtcDOav/9wE3tL09Suywb9sx8Dfgx8Aq1kL8HPAW8m5nHq8NmgM2Nzo+I7RExFRFT7SlZ0ko0M4zfANwIXAx8FjgL+EaDQ7PR+Zl5T2ZOZOZEK4VKak0zw/ivAS9l5tuZOQc8DHwZWF8N6wHGgNc7VKOkNmgm7K8AV0XEmRERwHXAPuBJ4ObqmG3Ao50pUVI7RGbD0fdHD4r4EfAHwHHgGeBPqc3RHwA2Vtv+KDOPLnOd5W8mqSWZGY22NxX2djHsUuctFXa/QScVwrBLhTDsUiEMu1QIwy4VwrBLhTDsUiEMu1QIwy4VwrBLhTDsUiEMu1QIwy4VwrBLhTDsUiG6HvbaH7uR1G7LZcueXSqEYZcKYdilQhh2qRA+oJM+JXxAJwlYhbD39fUREfbwUpvU89TXd+o427NLhehq2Jt595G0MvVR85L7u1iLpFXU9Z59cHCQvr4+e3ipTep5GhwctGeXBAPLH9I+/f39jI6OcuLECQAW/6eS8/Pz3SxFWtMWj4wHBwcBGB0d5dixY0uf0/GqJPWEroZ9eHiY8fFxRkZGGBkZYWBgYOHlPF5aXj0ni7NTz9Nll13G8PDw0ud2sU5Jq8iwS4Xo6gO60dFRtm7dytGjRwF48cUXF/YdOXIEYOEBw+KHd43WpU+zj3+EVl8fGhoCYGRkZGHfpZdeCsDWrVs5cODAkte0Z5cK0dWe/YILLuD2229f+Jht9+7dC/ump6cBOHz4MABzc3PAyY/k6stGPby9vtaiRl+AqW+rL/v7+4GPfrwGMD4+vnDO9ddfD8Dk5CQPPfTQkvezZ5cKEd3sFSPibeD/gINdu2lrzmHt1Aprq961VCusnXp/JzPPbbSjq2EHiIipzJzo6k1XaC3VCmur3rVUK6y9ehtxGC8VwrBLhViNsN+zCvdcqbVUK6ytetdSrbD26v2Ers/ZJa0Oh/FSIQy7VIiuhT0iro+I5yNiOiImu3XfZkXEhRHxZETsj4hnI+K2avvGiNgdEQeq5YbVrrUuIvoj4pmI2FWtXxwRe6paH4yIodWusS4i1kfEzoh4rmrjq3u1bSPiB9W/gb0R8fOIWNfLbdusroQ9IvqBfwC+AVwOfDciLu/GvU/DceCvM/N3gauAP6tqnASeyMxx4IlqvVfcBuxftL4DuLOq9R3g1lWpqrG7gccz83PAF6jV3XNtGxGbge8DE5n5eaAfuIXebtvmZGbHX8DVwK8Wrd8B3NGNe7dQ86PA14HngU3Vtk3A86tdW1XLGLWAXAvsAoLaN7wGGrX5Ktc6CrxE9UB40faea1tgM/AqsJHa747sAn6/V9v2dF7dGsbXG7BuptrWkyJiC3AFsAc4PzNnAarleatX2UfcBfwQqP/xvs8A72bm8Wq9l9r4EuBt4KfVtOPeiDiLHmzbzHwN+DHwCjALvAc8Re+2bdO6FfZGf9+2Jz/zi4gR4CHgLzPz8GrX00hEfAt4KzOfWry5waG90sYDwJeAf8zMK6j9fsSqD9kbqZ4b3AhcDHwWOIva9PPjeqVtm9atsM8AFy5aHwNe79K9mxYRg9SC/rPMfLja/GZEbKr2bwLeWq36FrkG+HZEvAw8QG0ofxewPiLqv7bcS208A8xk5p5qfSe18Pdi234NeCkz387MOeBh4Mv0bts2rVth/w0wXj3RHKL2wOOxLt27KVH7BeL7gP2Z+ZNFux4DtlU/b6M2l19VmXlHZo5l5hZqbfnrzPwe8CRwc3VYT9QKkJlvAK9GxGXVpuuAffRg21Ibvl8VEWdW/ybqtfZk256WLj74uAF4AXgR+NvVfljRoL6vUBua/Q/wX9XrBmpz4SeAA9Vy42rX+rG6vwrsqn6+BPhPYBr4F2B4tetbVOcXgamqfR8BNvRq2wI/Ap4D9gL/DAz3cts2+/LrslIh/AadVAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuF+H/HFs9oWhY8IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output_image.detach().numpy(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3, 608, 608)\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"test_set_images/\"\n",
    "test_images=[]\n",
    "for i in range(1, 51):\n",
    "    image_filename = root_dir + \"test_\" + str(i) + \"/test_\" + str(i) + '.png'\n",
    "    test_images.append(np.array(load_image(image_filename)).swapaxes(0,2).swapaxes(1,2))\n",
    "    test_images\n",
    "print(np.shape(test_images))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The file is bigger than 100*100! we need to cut it into 100*100 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = 100\n",
    "masks = []\n",
    "for test_image in test_images:\n",
    "    _, im_height, im_width = np.shape(test_image)\n",
    "    imgheight = test_image.shape[1]\n",
    "    imgwidth = test_image.shape[2]\n",
    "    mask = torch.zeros(1, imgheight, imgwidth)\n",
    "    for i in range(0, imgheight, crop):\n",
    "        for j in range(0, imgwidth, crop):\n",
    "            # when the crop is bigger than the image size, we increase the temporary image with 0\n",
    "            if(i+crop>imgheight and j+crop>imgwidth):\n",
    "                im_patch = np.zeros([3,crop,crop],dtype = np.float32)\n",
    "                im_patch[:, :imgheight-i, :imgwidth-j] = test_image[:, i:imgheight, j:imgwidth]\n",
    "                im_patch = Variable(torch.tensor(im_patch, requires_grad=True).unsqueeze(0))\n",
    "                mask[:, i:imgheight, j:imgwidth] = net(im_patch.float()).detach()[0,1,:imgheight-i,:imgwidth-j]\n",
    "            \n",
    "            elif(i+crop>imgheight):\n",
    "                im_patch = np.zeros([3,crop,crop],dtype = np.float32)\n",
    "                im_patch[:, :imgheight-i, :] = test_image[:, i:imgheight, j:j+crop]\n",
    "                im_patch = Variable(torch.tensor(im_patch, requires_grad=True).unsqueeze(0))\n",
    "                mask[:, i:imgheight, j:j+crop] = net(im_patch.float()).detach()[0,1,:imgheight-i,:]\n",
    "            \n",
    "            elif(j+crop>imgwidth):\n",
    "                im_patch = np.zeros([3,crop,crop])\n",
    "                im_patch[:, :, :imgwidth-j] = test_image[:, i:i+crop, j:imgwidth]\n",
    "                im_patch = Variable(torch.tensor(im_patch, requires_grad=True).unsqueeze(0))\n",
    "                mask[:, i:i+crop, j:imgwidth] = net(im_patch.float()).detach()[0,1,:,:imgwidth-j]\n",
    "            \n",
    "            else: # cas normal\n",
    "                im_patch = test_image[:, i:i+crop, j:j+crop]\n",
    "                im_patch = Variable(torch.tensor(im_patch, requires_grad=True).unsqueeze(0))\n",
    "                mask[:, i:i+crop, j:j+crop] = net(im_patch).detach()[0,1,:,:]\n",
    "    masks.append(mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def mask_to_submission_strings(image, img_number):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    patch_size = 16\n",
    "    for j in range(0, image.shape[1], patch_size):\n",
    "        for i in range(0, image.shape[2], patch_size):\n",
    "            patch = image[:,i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *images):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        i=int(0)\n",
    "        for image in images[0:]:\n",
    "            i+=1\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(image,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filename = 'submission.csv'\n",
    "masks_to_submission(submission_filename, *masks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
